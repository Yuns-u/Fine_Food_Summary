{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_food_reviews.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNj2Fs6cN0UAqsS3wd1+rwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuns-u/Fine_Food_Summary/blob/main/Amazon_food_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7xeqQWHO4kO"
      },
      "source": [
        "# Amazone의 음식 리뷰들을 요약추출해본다.\n",
        "\n",
        "앱스토어의 리뷰들을 추출요약하는 프로젝트를 해보려했으나 추출요약은 비지도학습을 위한 머신러닝모델을 만드는 것이 아니었다. 딥러닝을 보다 유용하게 사용해보기 위해서 생성요약을 하는 것을 통해 추후에 발전시키고자 한다.\n",
        "\n",
        "일단, 생성요약(abstractive summarization)을 통해 원문에 없는 문장이라도 핵심 문맥을 반영하여 원문을 요약하는 인공 신경망을 구축해보고자 한다. 이를 위해서 keras에서 원문과 실제 요약문인 레이블 데이터를 가지고 있는 데이터를 확보했고 이를 통해 학습시키고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuKLq3et6MFI"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl1sdD8MMwv8"
      },
      "source": [
        "#필요한 모듈 불러오기\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request\n",
        "\n",
        "np.random.seed(seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDVeHlqpQzkx"
      },
      "source": [
        "from google.colab import files\n",
        "myfiles = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdeqlSB4RHV5"
      },
      "source": [
        "# 데이터 불러오기\n",
        "\n",
        "해당 데이터가 매우 크기 때문에 10만개의 row에 대해서 데이터를 불러오고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX5cwHG9RA7N"
      },
      "source": [
        "df = pd.read_csv('Reviews.csv', nrows=100000)\n",
        "print('total reviews: ', len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKUTskcPWZDl"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIV0kq8SXJAf"
      },
      "source": [
        "생성요약을 위해서 필요한 feature는 리뷰의 원문인 text와 그에 대한 요약문인 summary이다. \n",
        "\n",
        "따라서 필요한 데이터인 text와 summary로 해당 데이터프레임을 재구성하여 데이터 프레임 자체의 차원의 수를 줄이고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrSa-ibYXjtT"
      },
      "source": [
        "df = df[['Text','Summary']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY8MZQSAXzFU"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VB3Hx6SYebI"
      },
      "source": [
        "# 중복 확인\n",
        "print('text열에서 중복이 아닌 데이터의 수: ', df['Text'].nunique())\n",
        "print('summary 열에서 중복이 아닌 데이터의 수: ', df['Summary'].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcZikJghY8hJ"
      },
      "source": [
        "요약문들은 짧다면 겹칠 가능성이 클 것이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKXkGdMwZn1Z"
      },
      "source": [
        "원문들은 대체로 길이가 길기 때문에 겹칠 가능성은 요약문보다 적을 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxToLv23ajJg"
      },
      "source": [
        "#text열에서 중복제거(원본데이터에서의 중복치도 지워주었다.)\n",
        "df.drop_duplicates(subset=['Text'], inplace=True)\n",
        "print('전체 데이터의 수: ', len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJuJBCavbNrK"
      },
      "source": [
        "#결측치 확인\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiwOPPpXbS3g"
      },
      "source": [
        "#결측치 처리\n",
        "#하나뿐이므로 직접 줄여서 넣어주어도 좋을 것 같다.\n",
        "\n",
        "#중복치가 없기 때문에 인덱스를 재설정해주는 것이 좋을 것이다.\n",
        "df.reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXhiEd_S2uu3"
      },
      "source": [
        "df.loc[df['Summary'].isna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnjs5sWq2THt"
      },
      "source": [
        "df['Text'].loc[33958]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsj9_UXG3oSq"
      },
      "source": [
        "임신을 해서 차를 마실 수 없다보니 두 세 개 밖에 안 먹었지만 나쁘지도 않고 좋지도 않았다고 한다. 따라서 Not a bad taste but not a big fan either로 요약하여 결측치를 대체하고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yGLWwKH4CSc"
      },
      "source": [
        "df['Summary'].loc[33958] = 'Not a bad taste but not a big fan of it either'\n",
        "df['Summary'].loc[33958]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFFAlNrn4gK9"
      },
      "source": [
        "df.loc[33958]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj9eqyPi4kaL"
      },
      "source": [
        "#결측치 재확인\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AiVHFDiePKo"
      },
      "source": [
        "# 전처리\n",
        "\n",
        "## 영문 약어를 정규화하기 위한 사전 만들기\n",
        "\n",
        "아래의 함수는 다음의 링크를 참조하여 만들어졌다.\n",
        "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CGXB2UfeTuQ"
      },
      "source": [
        "#전처리 함수 내 사용할 동의어 사전 만들기\n",
        "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
        "                \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \n",
        "                \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
        "                \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
        "                \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
        "                \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
        "                \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
        "                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
        "                \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
        "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
        "                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
        "                \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
        "                \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
        "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
        "                \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
        "                \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
        "                \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
        "                \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
        "                \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
        "                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
        "                \"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
        "                \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM0RPzih5vzk"
      },
      "source": [
        "## 불용어 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_utmMGhD5zGI"
      },
      "source": [
        "#NLTK의 불용어\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTkFm8B86gQ6"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print('불용어 개수: ',len(stop_words))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-r5-h1h5L6d"
      },
      "source": [
        "# 전처리 함수 설계\n",
        "def preprocess_sentence(sentence, remove_stopwords = True):\n",
        "    sentence = sentence.lower() # 텍스트 소문자화\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text #html 태그 제거\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) #괄호와 괄호 속의 문자열 제거\n",
        "    sentence = re.sub('\"','', sentence) #쌍따옴표 제거\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
        "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 's 제거\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) #영어 외 문자 공백\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. yummmmmmm->yumm\n",
        "\n",
        "    # 불용어 제거 (Text)\n",
        "    if remove_stopwords:\n",
        "        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
        "    # 불용어 미제거 (Summary)\n",
        "    else:\n",
        "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mbcC61Co1jp"
      },
      "source": [
        "# Text열 전처리하기\n",
        "cleaned_text = []\n",
        "\n",
        "for stce in df['Text']:\n",
        "  cleaned_text.append(preprocess_sentence(stce))\n",
        "\n",
        "cleaned_text[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syg-Uw19pcyK"
      },
      "source": [
        "# Summary열 전처리하기\n",
        "cleaned_summary = []\n",
        "\n",
        "for s in df['Summary']:\n",
        "  cleaned_summary.append(preprocess_sentence(s))\n",
        "\n",
        "cleaned_summary[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2iGWxlCrG4H"
      },
      "source": [
        "# 전처리된 결과물들을 데이터프레임에 저장하기\n",
        "\n",
        "df['Text'] = cleaned_text\n",
        "df['Summary'] = cleaned_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Ak5riVrUgv"
      },
      "source": [
        "# 전처리 과정에서 빈 값이 생겼다면 Null로 변환한 뒤 확인하기\n",
        "df.replace('', np.nan, inplace=True)\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUKLsApRrmrp"
      },
      "source": [
        "Summary 열에서 291개가 결측치가 있다. 결측치들을 제거한 뒤 전체 샘플 수를 확인해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxO5aV_ErwNx"
      },
      "source": [
        "df.dropna(axis=0, inplace=True)\n",
        "print('전체 데이터수: ', len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QL3dB6Rr70l"
      },
      "source": [
        "# 전처리된 텍스트들의 길이 분포 살펴보기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYawOJwcsBBW"
      },
      "source": [
        "# 길이 분포 출력\n",
        "text_len = [len(s.split()) for s in df['Text']]\n",
        "summ_len = [len(s.split()) for s in df['Summary']]\n",
        "\n",
        "print('원문의 최소 길이: ', np.min(text_len))\n",
        "print('원문의 최대 길이: ', np.max(text_len))\n",
        "print('원문의 평균 길이: ', np.mean(text_len))\n",
        "\n",
        "print('요약의 최소 길이: ', np.min(summ_len))\n",
        "print('요약의 최대 길이: ', np.max(summ_len))\n",
        "print('요약의 평균 길이: ', np.mean(summ_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O44zqwQsyx5"
      },
      "source": [
        "# 박스플롯으로 살펴보기\n",
        "\n",
        "# 요약문 텍스트 길이 분포 박스플롯\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(summ_len)\n",
        "plt.title('Text Length of Summary')\n",
        "plt.show()\n",
        "\n",
        "# 원문 텍스트 길이 분포 박스플롯\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(text_len)\n",
        "plt.title('Text Length of Original Text')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKZ9KzJLtkjg"
      },
      "source": [
        "# 히스토그램으로 살펴보기\n",
        "\n",
        "# 요약문 텍스트 길이 분포 히스토그램\n",
        "plt.title('Text Length of Summary')\n",
        "plt.hist(summ_len, bins=40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('the number of samples')\n",
        "plt.show()\n",
        "\n",
        "# 원문 텍스트 길이 분포 히스토그램\n",
        "plt.title('Text Length of Original Text')\n",
        "plt.hist(text_len, bins=40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('the number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxT5nElkubJP"
      },
      "source": [
        "원문 텍스트는 대체적으로 100이하의 길이를 가지며 평균적으로 38의 길이를 가지고 있다.\n",
        "\n",
        "요약의 경우 대체적으로 15이하의 길이를 가지며 평균 길이는 4이다.\n",
        "여기에서 패딩의 길이는 평균 길이보다 크게 잡아 60과 8로 정해주었다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ScekCSwDPY"
      },
      "source": [
        "text_max_len =60\n",
        "summ_max_len =8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX8CTrrQwHjK"
      },
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if (len(s.split()) <= max_len):\n",
        "      cnt = cnt+1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt/len(nested_list))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCm-PbzRwx58"
      },
      "source": [
        "below_threshold_len(text_max_len, df['Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJYZpGvYw35N"
      },
      "source": [
        "below_threshold_len(summ_max_len, df['Summary'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTL6EFRw82O"
      },
      "source": [
        "text열은 길이가 60 이하인 경우가 83%이고 summary 열은 길이가 8 이하인 경우가 99%로 정해준 최대 길이보다 큰 샘플들은 연산의 수월성을 위하여 제거하고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfVDYUdH8B9L"
      },
      "source": [
        "df = df[df['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biQTwpZY8ShZ"
      },
      "source": [
        "df = df[df['Summary'].apply(lambda x: len(x.split()) <= summ_max_len)]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eELnQRT5xmEp"
      },
      "source": [
        "print('전체 샘플수: ',len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEWka5NExu7Y"
      },
      "source": [
        "# seq2seq 훈련을 위한 준비\n",
        "seq2seq 훈련을 하기 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가해주어야 한다. 시작토큰을 'sijaktoken', 종료 토큰은 'jongryotoken'이라 만들어 앞뒤로 추가해주었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJuK2_7cyKZb"
      },
      "source": [
        "#시작토큰과 종료토큰 앞뒤로 붙여주기\n",
        "df['decoder_input'] = df['Summary'].apply(lambda x: 'sijaktoken '+x)\n",
        "df['decoder_target'] = df['Summary'].apply(lambda x: x+' jongryotoken')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwbPfZ-0yoiu"
      },
      "source": [
        "#인코더의 입력, 디코더의 입력과 레이블을 각각 정해주어 array로 만들어주기\n",
        "encoder_input = np.array(df['Text'])\n",
        "decoder_input = np.array(df['decoder_input'])\n",
        "decoder_target = np.array(df['decoder_target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGu6egpJy8Gr"
      },
      "source": [
        "# 데이터 분리\n",
        "\n",
        "훈련데이터와 테스트 데이터를 분리하기 위해 먼저 순서가 섞인 정수 시퀀스를 만들어준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uV-bgHszH-I"
      },
      "source": [
        "#순서를 섞어 정수 시퀀스를 만들어주기\n",
        "indicies = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indicies)\n",
        "print(indicies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaBmb-YE9MCo"
      },
      "source": [
        "이렇게 랜덤으로 섞인 시퀀스를 데이터의 샘플 순서로 정의하여 샘플의 순서를 섞이게 할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfHDfeBX9S0B"
      },
      "source": [
        "encoder_input = encoder_input[indicies]\n",
        "decoder_input = decoder_input[indicies]\n",
        "decoder_target = decoder_target[indicies]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQzPMREz9oFK"
      },
      "source": [
        "일반적으로 훈련데이터와 테스트데이터는 8:2로 나누므로 그렇게 나누어준다.\n",
        "정제된 전체 데이터의 20%는 14673라고 할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnJLQ9Nc9vKQ"
      },
      "source": [
        "test_num = int(len(indicies)*0.2)\n",
        "test_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFQmupk-Kqs"
      },
      "source": [
        "train_encoder_input = encoder_input[:-test_num]\n",
        "train_decoder_input = decoder_input[:-test_num]\n",
        "train_decoder_target = decoder_target[:-test_num]\n",
        "\n",
        "test_encoder_input = encoder_input[-test_num:]\n",
        "test_decoder_input = decoder_input[-test_num:]\n",
        "test_decoder_target = decoder_target[-test_num:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oCyE_jO-o4u"
      },
      "source": [
        "print('훈련데이터의 개수: ', len(train_encoder_input))\n",
        "print('훈련레이블의 개수: ', len(train_decoder_input))\n",
        "print('테스트데이터의 개수: ', len(test_encoder_input))\n",
        "print('테스트레이블의 개수: ', len(test_decoder_input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx9R3AJz-ntg"
      },
      "source": [
        "# 정수인코딩\n",
        "텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 해야한다.\n",
        "훈련데이터에 대해서 단어 집합을 만들어야 하는데 먼저 원문에 해당하는 encoder_input_train에 대해서 진행해 단어집합의 크기를 어느 정도로 설정하면 좋을지 판단해볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4DG-ZEY_oTD"
      },
      "source": [
        "src_tokenizer = Tokenizer()\n",
        "src_tokenizer.fit_on_texts(train_encoder_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M75hJj-k_2mf"
      },
      "source": [
        "단어집합이 생성되면서 각 단어에 고유한 정수가 붙었다. \n",
        "\n",
        "각 단어의 고유 정수는 `src_tokenizer.word_index`에 저장되어 있다.\n",
        "\n",
        "먼저 빈도수가 낮은 단어들은 자연어 처리에서 배제하고자 한다.\n",
        "빈도수가 10 미만인 단어들이 이 데이터에서 얼마만큼의 비중을 차지하는지 확인해볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJOdjY6SAQBF"
      },
      "source": [
        "threshold = 10\n",
        "total_cnt = len(src_tokenizer.word_index) #단어의 개수\n",
        "rare_cnt = 0 #빈도수가 threshold보다 작은 개수를 센다.\n",
        "total_freq = 0 #훈련데이터의 전체 단어 빈도수 총합\n",
        "rare_freq = 0 #등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총합\n",
        "\n",
        "#key와 value로  단어와 빈도수의 쌍(pair)을 만들어준다.\n",
        "for key, value in src_tokenizer.word_counts.items():\n",
        "  total_freq = total_freq+value\n",
        "\n",
        "  #단어의 등장 빈도수가 threshold보다 작으면\n",
        "  if(value < threshold):\n",
        "    rare_cnt = rare_cnt+1\n",
        "    rare_freq = rare_freq + value\n",
        "\n",
        "print('단어집합(vocabulary)의 크기: ',total_cnt)\n",
        "print('등장빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold-1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt-rare_cnt))\n",
        "print('단어집합에서 희귀 단어의 비율: ',(rare_cnt/total_cnt)*100)\n",
        "#print('전체 등장빈도에서 희귀 단어 등장 빈도비율: ',(rare_\bfreq/total_freq)*100)\n",
        "print('전체 등장빈도에서 희귀 단어 등장 빈도비율: ',0.03874439358357905*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dG0s6LED7kC"
      },
      "source": [
        "등장 빈도가 threshold 값인 10회 미만, 즉 9회 이하인 단어드은 단어집합에서 78%를 차지하지만 실제 훈련데이터에서 등장빈도로 차지하는 비중은 3.8% 정도 밖에 되지 않는다.\n",
        "\n",
        "따라서 등장빈도가 10회 미만인 단어들은 정수 인코딩 과정에서 배제하고자 한다.\n",
        "위와 비슷한 단어 집합의 크기는 7652이므로 단어집합의 크기를 7652로 설정하고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagtYdXeF94T"
      },
      "source": [
        "src_vocab = 7652\n",
        "src_tokenizer = Tokenizer(num_words=src_vocab)\n",
        "src_tokenizer.fit_on_texts(train_encoder_input)\n",
        "\n",
        "#텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "train_encoder_input = src_tokenizer.texts_to_sequences(train_encoder_input)\n",
        "test_encoder_input = src_tokenizer.texts_to_sequences(test_encoder_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggcK3uQ4HlSa"
      },
      "source": [
        "print(train_encoder_input[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lJKfO3XHwUb"
      },
      "source": [
        "#레이블에게도 텍스트 시퀀스를 정수시퀀스로 변환\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(train_decoder_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNMvtF-nHwAU"
      },
      "source": [
        "레이블에 해당하는 summary 데이터에도 단어집합이 생성되면서 각 단어에 고유한 정수가 부여되었다. 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼마만큼의 비중을 차지하는지 다시 살펴보고자 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX71M_SAIqC8"
      },
      "source": [
        "threshold = 6\n",
        "total_cnt = len(tar_tokenizer.word_index) #단어의 개수\n",
        "rare_cnt = 0 #빈도수가 threshold보다 작은 개수를 센다.\n",
        "total_freq = 0 #훈련데이터의 전체 단어 빈도수 총합\n",
        "rare_freq = 0 #등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총합\n",
        "\n",
        "#key와 value로  단어와 빈도수의 쌍(pair)을 만들어준다.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "  total_freq = total_freq+value\n",
        "\n",
        "  #단어의 등장 빈도수가 threshold보다 작으면\n",
        "  if(value < threshold):\n",
        "    rare_cnt = rare_cnt+1\n",
        "    rare_freq = rare_freq + value\n",
        "\n",
        "print('단어집합(vocabulary)의 크기: ',total_cnt)\n",
        "print('등장빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold-1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt-rare_cnt))\n",
        "print('단어집합에서 희귀 단어의 비율: ',(rare_cnt/total_cnt)*100)\n",
        "#print('전체 등장빈도에서 희귀 단어 등장 빈도비율: ',(rare_\bfreq/total_freq)*100)\n",
        "print('전체 등장빈도에서 희귀 단어 등장 빈도비율: ',0.07630141772245003*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ow1MMPPJT3G"
      },
      "source": [
        "등장빈도가 5회 이하인 단어들은 단어 집합에서 약 77%를 차지하지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 7%정도로 매우 낮다. 이 단어들을 정수 인코딩 과정에서 배제할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-JD2kZrI3xR"
      },
      "source": [
        "tar_vocab = 2553\n",
        "tar_tokenizer = Tokenizer(num_words=tar_vocab)\n",
        "tar_tokenizer.fit_on_texts(train_decoder_input)\n",
        "tar_tokenizer.fit_on_texts(train_decoder_target)\n",
        "\n",
        "#텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "train_decoder_input = tar_tokenizer.texts_to_sequences(train_decoder_input)\n",
        "train_decoder_target = tar_tokenizer.texts_to_sequences(train_decoder_target)\n",
        "test_decoder_input = tar_tokenizer.texts_to_sequences(test_decoder_input)\n",
        "test_decoder_target = tar_tokenizer.texts_to_sequences(test_decoder_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmy6t0qVM24i"
      },
      "source": [
        "#정수 인코딩이 잘 되었는지 훈련 데이터에 대해서 샘플 추출해서 확인하기\n",
        "print(train_decoder_input[:3])\n",
        "print(train_decoder_target[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXrdQKrMM1ta"
      },
      "source": [
        "# 빈 샘플 제거\n",
        "빈도수가 낮은 단어만으로 구성되었던 샘플은 이제 빈 샘플이 되었다.\n",
        "이 현상은 길이가 상대적으로 짧았던 요약문의 경우 빈 샘플이 되어버린 경우가 많을 것이다.\n",
        "\n",
        "요약문에서 0이 된 값들의 인덱스를 받아와서 삭제를 해줘야할 것이다.\n",
        "\n",
        "이 때, 요약문인 decoder_input과 decoder_target에는 시작토큰과 종료토큰이 추가된 상태이다. 이 두 토큰은 모든 데이터에 있기 때문에 단어집합 제한에도 삭제되지 않는다. 따라서 길이가 0인 요약문의 실제 길이는 1이된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4MqDxWnNv5I"
      },
      "source": [
        "drop_train = [index for index, sentence in enumerate(train_decoder_input) if len(sentence) == 1]\n",
        "drop_test = [index for index, sentence in enumerate(test_decoder_input) if len(sentence) == 1]\n",
        "\n",
        "print('삭제할 훈련 데이터의 개수:',len(drop_train))\n",
        "print('삭제할 테스트 데이터의 개수:',len(drop_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjX783-NF9I2"
      },
      "source": [
        "삭제해야하는 데이터들을 삭제하여 각각의 개수를 구하면 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BjzfKEXOcuP"
      },
      "source": [
        "train_encoder_input = np.delete(train_encoder_input, drop_train, axis=0)\n",
        "train_decoder_input = np.delete(train_decoder_input, drop_train, axis=0)\n",
        "train_decoder_target = np.delete(train_decoder_target, drop_train, axis=0)\n",
        "\n",
        "test_encoder_input = np.delete(test_encoder_input, drop_test, axis=0)\n",
        "test_decoder_input = np.delete(test_decoder_input, drop_test, axis=0)\n",
        "test_decoder_target = np.delete(test_decoder_target, drop_test, axis=0)\n",
        "\n",
        "print('훈련 데이터의 개수:',len(train_encoder_input))\n",
        "print('훈련 레이블의 개수:',len(train_decoder_input))\n",
        "print('테스트 \b데이터의 개수:',len(test_encoder_input))\n",
        "print('테스트 \b레이블의 개수:',len(test_decoder_input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLl4qVOUQM8J"
      },
      "source": [
        "# 패딩하기\n",
        "앞의 최대 길이로 맞춰 훈련데이터와 테스트 데이터에 대해서 패딩을 하면 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGVnWPQQSuE"
      },
      "source": [
        "train_encoder_input = pad_sequences(train_encoder_input, maxlen = text_max_len, padding='post')\n",
        "test_encoder_input = pad_sequences(test_encoder_input, maxlen = text_max_len, padding='post')\n",
        "train_decoder_input = pad_sequences(train_decoder_input, maxlen = summ_max_len, padding='post')\n",
        "train_decoder_target = pad_sequences(train_decoder_target, maxlen = summ_max_len, padding='post')\n",
        "test_decoder_input = pad_sequences(test_decoder_input, maxlen = summ_max_len, padding='post')\n",
        "test_decoder_target = pad_sequences(test_decoder_target, maxlen = summ_max_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShaNFGEvRAiY"
      },
      "source": [
        "# seq2seq와 attention으로 요약 모델 설계 및 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_oRJ-FVRIVs"
      },
      "source": [
        "#필요한 모듈 불러오기\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I0CbSTIRmzZ"
      },
      "source": [
        "## 인코더 설계\n",
        "인코드는 LSTM을 3층을 쌓았다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tyAh4CORliO"
      },
      "source": [
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "\n",
        "#인코더\n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "#인코더의 임베딩 층\n",
        "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "#인코더의 LSTM 1층\n",
        "encoder_lstm_1 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output_1, state_h1, state_c1 = encoder_lstm_1(enc_emb)\n",
        "\n",
        "#인코더의 LSTM 2층\n",
        "encoder_lstm_2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_output_2, state_h2, state_c2 = encoder_lstm_2(enc_emb)\n",
        "\n",
        "#인코더의 LSTM 3층\n",
        "encoder_lstm_3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm_3(enc_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqNjMGepSxdn"
      },
      "source": [
        "## 디코더 설계\n",
        "디코더의 설계는 인코더와 사실상 동일하지만 초기상태를 인코더의 상태로 주어야한다.\n",
        "출력층은 제외하고 설계한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0TSTZzBS9I8"
      },
      "source": [
        "#디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#디코더의 임베딩 층\n",
        "dec_emb_layer= Embedding(tar_vocab, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "#디코더의 LSTM\n",
        "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLAc1OYUk7b"
      },
      "source": [
        "디코더 출력층 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTkecYJKUnkp"
      },
      "source": [
        "#디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDoc5aiKU4M3"
      },
      "source": [
        "# 모델 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNPxA2JhU6Y0"
      },
      "source": [
        "#모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAAisdfCY9jx"
      },
      "source": [
        "#위의 모델을 compile하여 써보고자 한다.\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTjA1F8ZO8J"
      },
      "source": [
        "조기종료조건을 설정하고 모델을 학습시키면 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KPmpkM0ZUIZ"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
        "history = model.fit(x = [train_encoder_input, train_decoder_input], y = train_decoder_target, \\\n",
        "          validation_data = ([test_encoder_input, test_decoder_input], test_decoder_target),\n",
        "          batch_size = 256, callbacks=[es], epochs = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI7fQjHaZ7qJ"
      },
      "source": [
        "학습 과정에서 기록된 훈련 데이터의 손실과 테스트 데이터의 손실 히스토리를 시각화하면 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB64hXCuaBD3"
      },
      "source": [
        "plt.plot(history.history['loss'],label='train')\n",
        "plt.plot(history.history['val_loss'],label='test')\n",
        "plt.xlabel('number of epochs')\n",
        "plt.ylabel('value of loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnv1avN-dUi2"
      },
      "source": [
        "epoch 5 정도에서 손실이 가장 적은 것을 알 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIk0TVqMf_HF"
      },
      "source": [
        "# -------------확인----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_sMcpD2gkVK"
      },
      "source": [
        "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
        "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
        "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwDnb4Dub3Yf"
      },
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2text(input_seq):\n",
        "    sentence=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            sentence = sentence + src_index_to_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2summary(input_seq):\n",
        "    sentence=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=tar_word_to_index['sijaktoken']) and i!=tar_word_to_index['jongryotoken']):\n",
        "            sentence = sentence + tar_index_to_word[i] + ' '\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBD9q5n_hfmx"
      },
      "source": [
        "# 인코더 설계\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seeNdbC3h5lh"
      },
      "source": [
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ToTxrAjFfp"
      },
      "source": [
        "# 어텐션 함수\n",
        "#decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
        "#attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "#decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "#어텐션 함수 적용 전 디코더의 출력층\n",
        "#decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "#decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
        "\n",
        "# 최종 디코더 모델\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD2-KA0gjGmS"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6mqRzmOgzdM"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "     # <SOS>에 해당하는 토큰 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_word_to_index['sijaktoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tar_index_to_word[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='jongryotoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_token == 'jongryotoken'  or len(decoded_sentence.split()) >= (summ_max_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 상태를 업데이트 합니다.\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOFPbX7FgC1q"
      },
      "source": [
        "for i in range(500, 510):\n",
        "    print(\"원문 : \",seq2text(test_encoder_input[i]))\n",
        "    print(\"실제 요약문 :\",seq2summary(test_decoder_input[i]))\n",
        "    print(\"예측 요약문 :\",decode_sequence(test_encoder_input[i].reshape(1, text_max_len)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tAwDloeaZH4"
      },
      "source": [
        "# ---------아직 이해 및 적용 안됨------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTMnHA9gVVfS"
      },
      "source": [
        "# 어텐션 매커니즘 사용해보기\n",
        "어텐션 메커니즘을 사용하기 위해서는 설꼐한 출력층을 사용하지 않고 어텐션 메커니즘이 결합된 새로운 출력층을 만들어 볼 수 있다. 어텐션 함수를 직접 작성하지 않고 깃허브에 공개된 함수를 사용할 수 있어서 아래의 코드를 통해 attention.py를 다운로드하고 attention layer를 불러올수 있다. 해당 어텐션은 바다나우 어텐션이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dECChCvVS4a"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
        "from attention import AttentionLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI2FcZHHXAQc"
      },
      "source": [
        "#어텐션 층(어텐션 함수)\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "#어텐션의 결과와 디코더의 hidden state들을 연결\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#디코더의 출력층\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "#모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}